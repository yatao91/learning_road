## Spark运行模式及对应安装步骤

#### 一.本地部署运行模式(单机)

- 适用场景: 本地开发调试适用的环境

- 部署步骤: 

  1. 安装`JDK1.8`以上版本, 并将`JAVA_HOME`环境变量写到当前用户shell下的文件中, 如`.zshrc`或`.bashrc`.

     [CentOS7 使用yum命令安装Java SDK(openjdk)](https://blog.csdn.net/youzhouliu/article/details/51183115)

  2. 安装Python环境, 我这里用的是`Miniconda3`.

  3. 在[spark下载页面](https://spark.apache.org/downloads.html)下载所需版本的安装包. 这里可以选择`spark-2.4.4-bin-hadoop2.7.tgz`这个版本. 虽然可能有些人并不使用`hadoop`, 仅使用磁盘存储, 也不用担心选择这个版本, 因为可以不用去部署`hadoop`的.(不仅不用部署`hadoop`, 同样`spark`依赖的`Scala`也不需要去安装, 因为`spark`安装好以后, 自带了`Scala`环境).

  4. `centos`系统中新建账户`spark`, 然后将此压缩包放至某个文件夹--按个人习惯即可.解压到这个文件夹即可. 如果嫌名称过长, 可以修改一下:

     ```shell
     mv spark-2.4.4-bin-hadoop2.7 spark
     ```

  5. 需要修改下`spark`中的配置文件`spark-env.sh`. 

     ```shell
     cd spark/conf
     
     mv spark-env.sh.template spark-env.sh
     
     # 在spark-env.sh中增加如下内容:
     SPARK_MASTER_HOST=spark001  # 绑定spark master节点到spark001地址
     SPARK_MASTER_PORT=7077  # master监听端口
     export JAVA_HOME=/usr/java/jdk1.8.0_211-amd64/  # 指定Java地址
     export PYSPARK_PYTHON=/home/spark/miniconda3/bin/python3  # 指定PySpark的driver和worker使用的Python地址
     export PYSPARK_DRIVER_PYTHON=/home/spark/miniconda3/bin/python3  # 指定仅PySpark的driver使用的Python地址
     ```

     > 注意: 虽然local模式下, 将`PYSPARK_PTHON`及`PYSPARK_DRIVER_PYTHON`环境变量放到`bash`环境变量下可以生效, 但在`StandAlone`模式下, 提交程序后将会报错:
     >
     > ```
     > Exception: Python in worker has different version 3.4 than that in driver 2.7, PySpark cannot run with different minor versions
     > ```
     >
     > 所以上述设置时, 将其设置到`spark`的配置文件`spark-env.sh`中, 在分发到各节点时, 即各节点的环境变量都得到了统一配置.

  6. 配置用户环境变量, 在用户家目录下的`.bashrc`或`.zshrc`文件中做如下配置:

     ```shell
     vim .bashrc
     
     export SPARK_HOME=/home/spark/spark
     export PATH=$SPARK_HOME/bin:$PATH
     export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH  # 为了在python3中引入pyspark库
     ```

     以上配置目的是在`python3`中引入`pyspark`库.

     > 同时, 如果不以这种方式指定, 也可以通过以下方式手动安装:
     >
     > ```shell
     > pip install pyspark
     > ```
     >
     > 不过, 如果是集群模式,那么需要保证每个集群下均存在这个库 简单的方式就是直接引入`spark`中本身带有的`pyspark`库. 不过也是因人而异.

- 本地模式下`spark`使用

  1. 验证`spark`是否安装成功

     通过运行如下示例, 验证`spark`是否安装成功

     ```shell
     cd /home/spark/spark
     ./bin/run-example SparkPi
     ```

     执行后, 如果没有报错, 并且返回信息中出现以下计算结果:

     ```
     Pi is roughly 3.1425357126785634
     ```

     即表示`spark`安装成功.

  2. 使用`pyspark`运行代码

     通过以下命令进入`pyspark`交互环境

     ```shell
     cd /home/spark/spark
     ./bin/pyspark
     ```

     > 上面配置无问题的话, 是可以正常启动的. 

     `pyspark`命令及其常用参数如下:

     ```shell
     ./bin/pyspark --master <master-url>
     ```

     `spark`的运行模式取决于传递给`SparkContext`中的`Master URL`的值. `Master URL`可以是以下任一种形式:

     - `local`: 使用一个`worker`线程本地化运行`spark`(完全不并行)
     - `local[*]`: 使用逻辑CPU个数数量的线程来本地化运行`spark`
     - `local[K]`: 使用K个`worker`线程本地化运行`spark`(理想情况下, K应该根据机器的CPU核数设定)
     - `spark://HOST:PORT`: 连接到指定的`spark standalone master` 默认端口是7077
     - `yarn-client`: 以客户端形式连接YARN集群.
     - `yarn-cluster`:以集群模式连接YARN集群.
     - `mesos://HOST:PORT`: 连接到指定的`Mesos`就请你. 默认端口是5050.

     当前我们采用的是本地模式, 即`local`模式.

     在`spark`中采用本地模式启动`pyspark`的命令主要包含以下参数:

     - `--master`: 表示当前`pyspark`连接到哪个`master` 如果是`local[*]`, 就是使用本地模式启动`pyspark`, 其中括号中的星号表示需要使用几个CPU核心.
     - `--jars`: 这个参数用于把相关的JAR包添加到`CLASSPATH`中; 如果有多个包, 可以使用逗号分隔符连接它们.

     比如: 要采用本地模式, 在4个CPU核心上运行`pyspark`:

     ```shell
     cd /home/spark/spark
     ./bin/pyspark --master local[4]
     ```

     或者, 可以在`CLASSPATH`中添加jar包, 如下所示:

     ```shell
     ./bin/pyspark --master local[4] --jars somejar.jar
     ```

  3. `spark`独立应用程序编程--`pyspark`

     示例代码如下:

     ```python
     from pyspark import SparkContext
     sc = SparkContext( 'local', 'test')
     logFile = "/home/spark/spark/README.md"
     logData = sc.textFile(logFile, 2).cache()
     numAs = logData.filter(lambda line: 'hardware' in line).count()
     numBs = logData.filter(lambda line: 'b' in line).count()
     print('Lines with hardware: %s, Lines with b: %s' % (numAs, numBs))
     ```

     保存代码后可以通过以下两种方式运行:

     ```shell
     # 1.直接通过python执行代码
     python test.py
     
     # 2.通过spark-submit提交代码执行
     spark-submit --master local[*] test.py
     ```

     执行结果如下所示:

     ```shell
     20/01/18 05:31:52 WARN Utils: Your hostname, spark004 resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface eth0)
     20/01/18 05:31:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
     20/01/18 05:31:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
     Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
     Setting default log level to "WARN".
     To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
     
     Lines with a: 62, Lines with b: 31
     ```

  以上就是本地模式下`spark`的部署与使用的情况.

  #### 二.standalone部署运行模式

  1. 简介

     与`MapReduce1.0`框架类似, `spark`框架本身也自带了完整的资源调度管理服务, 可以独立部署到一个集群中, 而不需要依赖其他系统为其提供资源管理调度服务. 在架构的设计上, `spark`和`MapReduce1.0`完全一致, 都是由一个`Master`和若干个`Slave`构成, 并且以槽(slot)作为资源分配单位. 不同的是, `spark`中的槽不再像`MapReduce1.0`那样分为`Map`槽和`Reduce`槽, 而是只设计了统一的一种槽提供给各种任务来使用.

  2. 部署

     这里采用3台机器(节点)作为实例.一台机器为Master节点, 另外两台为Slave节点(即worker节点).

     - 需配置ssh无密码登录. spark集群中各节点的通信需要ssh协议进行.

       1. 在集群各节点spark用户下执行如下命令生成ssh密钥对(一路回车即可):
     
          ```shell
          # spark @ spark in ~ [14:48:03] 
     $ ssh-keygen -t rsa
          ```

       2. 将master注册到各slave节点上.且将各slave节点注册到master上.
     
          即将master节点ssh公钥写入到各slave节点`authorized_keys`文件中
     
          将slave节点ssh公钥写入到master节点`authorized_keys`文件中.
     
       3. 以上配置后便可实现ssh无密登录.

     - 在Master节点机器上, 下载spark安装包. 下载完成后, 执行如下命令.

       ```shell
  tar -xzvf spark-2.4.4-bin-hadoop2.7.tgz
       mv spark-2.4.4-bin-hadoop2.7 spark
       ```
     
     - 在用户家目录下配置环境变量:
     
       ```shell
  vim .bashrc
       
  export SPARK_HOME=/home/spark/spark  # spark目录
       export PATH=$SPARK_HOME/bin:$PATH
       export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH  # 为了在python3中引入pyspark库
       ```
     
     - spark配置:
     
       - 配置slaves文件
     
         ```shell
    [spark@spark004 conf]$ cp slaves.template slaves
         [spark@spark004 conf]$ vim slaves
    slave01  # 节点01
         slave02  # 节点02
    ```
     
  - 配置spark-env.sh文件
     
    ```shell
         cp spark-env.sh.template spark-env.sh
         vim spark-env.sh
         
    SPARK_MASTER_HOST=master  # 绑定spark master地址
         SPARK_MASTER_PORT=7077  # master监听端口
    SPARK_MASTER_WEBUI_PORT=8060  # master web端口
         SPARK_WORKER_WEBUI_PORT=8061  # worker web端口
         SPARK_WORKER_CORES=8  # worker可用核数
       SPARK_WORKER_MEMORY=4096m  # worker可用内存
         export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.232.b09-0.el7_7.x86_64/jre/  # 指定Java地址
       export PYSPARK_PYTHON=/home/spark/miniconda3/bin/python3  # 指定PySpark的driver和worker使用的Python地址
         ```

         > 注意: 不同JDK安装方式, 在配置JAVA_HOME时有所不同. 当前使用`yum`形式安装的`openjdk`, 在配置JAVA_HOME时, 需指向`/jre`路径.
  >
         > 如果JAVA_HOME已经在系统里配置并激活, 可以不用在`spark-env.sh`再次指定.

       - 配置好后, 将spark文件夹分发到各节点. 可以使用[xsync](https://www.twblogs.net/a/5ca225cdbd9eee5b1a06c3d2/zh-cn)来进行集群间的文件同步工作.
       
       - 启动spark集群

         1. 启动Master节点

            在Master节点上运行如下命令:
       
            ```shell
            ./spark/sbin/start-master.sh
            ```
       
     启动后在master节点上运行jps命令, 可以发现多了个Master进程:
       
     ```shell
            # spark @ spark001 in ~/spark/sbin [14:11:12] 
     $ jps
            25484 Jps
       25358 Master
            ```

         2. 启动所有Slave节点
     
            在master节点上运行如下命令:
       
       ```shell
            ./spark/sbin/start-slaves.sh
          ```
       
        在各slave节点上运行如下命令, 可以看到多了个worker进程:
       
        ```shell
            # spark @ spark002 in ~ [14:11:14] 
          $ jps
            18769 Worker
          18990 Jps
            ```
     
         3. 在浏览器上查看spark standalone集群管理器的集群信息
       
            访问master:8080即可.
       
     - 关闭spark集群
       
         在master节点上执行如下命令:
       
         ```shell
         ./sbin/stop-master.sh
         ./sbin/stop-slaves.sh
         ```
       
         
  
  
  
  
  
  

