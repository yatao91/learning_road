## pyspark读取es数据

#### 使用sparkSQL读取

```python
# -*- coding: utf-8 -*-
import os
from pyspark.sql import SparkSession

os.environ["PYSPARK_PYTHON"] = "/home/spark/spark/miniconda3/bin/python3"

spark = SparkSession.builder.master("spark://spark001:7077").appName("test_production_es_data").getOrCreate()

df = spark.read\
    .format("org.elasticsearch.spark.sql")\
    .option("es.nodes", "192.168.0.111")\
    .option("es.port", "9288")\
    .option("es.resource", "index/doc")\
    .load()
```

#### 使用RDD模式读取

```python
# -*- coding: utf-8 -*-
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("spark://spark001:7077").setAppName("demo")
sc = SparkContext(conf=conf)

# es相关配置
es_conf = {
    "es.nodes": "192.168.0.111",               
    "es.port": "9288",        
    "es.resource": "index/doc"
}

rdd_01 = sc.newAPIHadoopRDD(
    inputFormatClass="org.elasticsearch.hadoop.mr.EsInputFormat",
    keyClass="org.apache.hadoop.io.NullWritable",
    valueClass="org.elasticsearch.hadoop.mr.LinkedMapWritable",
    conf=es_conf
)

```

#### 使用query读取es数据

```python
from pyspark import SparkConf
from pyspark.sql import SQLContext

conf = SparkConf().setAppName("ESTest")
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)

# 注意:query语句使用文档字符串形式
q ="""{
  "query": {
    "filtered": {
      "filter": {
        "exists": {
          "field": "label"
        }
      },
      "query": {
        "match_all": {}
      }
    }
  }
}"""

es_read_conf = {
    "es.nodes" : "localhost",
    "es.port" : "9200",
    "es.resource" : "titanic/passenger",
    "es.query" : q
}

es_rdd = sc.newAPIHadoopRDD(
    inputFormatClass="org.elasticsearch.hadoop.mr.EsInputFormat",
    keyClass="org.apache.hadoop.io.NullWritable", 
    valueClass="org.elasticsearch.hadoop.mr.LinkedMapWritable", 
    conf=es_read_conf)

sqlContext.createDataFrame(es_rdd).collect()
```

