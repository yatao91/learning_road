## spark读取es中多值或array字段时的处理

#### 概述

ES中对单值或多值字段的处理方式是相同的. 而且, 在ES的`mapping`中并无信息表明单值或多值的信息. 使用客户端读取ES数据时, 便在实际读取之前无法分辨一个字段是单值还是多值.

在大多数情况下, 这不是一个问题, `elasticsearch-Hadoop`自动创建必要的`array/list`.

但在`sparkSQL`下, 有严格的`schema`(模式), 不允许从字段的声明类型更改字段的实际值. 更糟糕的是, 这些信息在读取数据之前就需要可用. 

因为`mapping`不是足够明确的, `elasticsearch-hadoop`允许用户指定多余的信息来表名具体某些字段需要以`array/list`类型读取.

#### 配置选项

- `es.read.field.as.array.include(default empty)`

  定义在此选项下的字段被当做`array/list`来处理. 

  因为`elasticsearch`可以映射一个或多个值到一个字段上, `elasticsearch-hadoop`无法从映射中确定是实例化一个值还是数组类型. 

  当遇到多值情况时, `elasticsearch-hadoop`将自动使用`array/list`类型, 但在严格的映射场景下(比如`sparkSQL`)这可能导致意外的`schema`改变. 

  这个配置选项使用方法类似于`elasticsearch`的`include/exclude`. 多个字段可以使用逗号分隔指定. 默认情况下, 没有字段被指定意味着没有字段被包含. 

  ```
  es.read.field.as.array.include = a,b,a.name,a.age
  ```

- `es.read.field.as.array.exclude(default empty)`

  定义在此选项下的字段不被当做`array/list`来处理. 

  使用方式与`es.read.field.as.array.include`一致. 

  默认情况下, 没有指定字段被排除时, 而且没有字段被包含在`include`中, 则没有字段被当做`array`类型.

#### 示例

```python
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# 指定pyspark使用的Python
os.environ["PYSPARK_PYTHON"] = "/home/spark/spark/miniconda3/bin/python3"

# 构造spark session
spark = SparkSession.builder.master("spark://spark001:7077").appName("test_production_es_data").getOrCreate()

"""
读取es数据,指定所需es option
"""
es_nodes = "192.168.0.111"
es_port = "9288"
es_resource = "test/doc"
es_exclude = "id,manual_check,html_text,deduplication,agency_contact,cursor," \
"deduplication_weight,content,timestamp,status,sniffer_time_cursor,bid_company_alliance,owner_contact,project_contact,title"

# 指定作为array处理的字段, 此处是agency owner两个字段
es_array_include = "agency,owner"

df = spark.read \
    .format("org.elasticsearch.spark.sql") \
    .option("es.nodes", es_nodes) \
    .option("es.port", es_port) \
    .option("es.resource", es_resource) \
    .option("es.read.field.exclude", es_exclude) \
    .option("es.read.field.as.array.include", es_array_include) \
    .load()
```

