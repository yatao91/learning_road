## sparkSQL中explode与explode_outer区别

#### 问题背景

因业务数据中存在`array`类型的字段, 需要先进行打平操作, 然后进行后续临时表的创建. 

上一节, 采用的解决方案是使用函数`explode`进行打平操作, 但使用此函数后, 最终处理数据时, 发现数据量与实际不符. 缺少了很多数据.

#### 问题解析

经过追踪, 发现业务数据中的`array`类型字段并非每条数据中都存在, 有些并没有此字段, 导致`explode`打平数组字段时, 自动忽略这些没有数组字段的数据. 导致数据量的减少. 经过查阅spark源码及官方文档, 发现另外一个进行打平操作的函数`explode_outer`.

首先, 我们看下`explode`与`explode_outer`的区别:

官方API文档中的介绍如下:

- `explode`:

  ```python
  # Returns hardware new row for each element in the given array or map.
  # 对数组中的每个元素返回一个新行
  
  >>> from pyspark.sql import Row
  >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={"hardware": "b"})])
  >>> eDF.select(explode(eDF.intlist).alias("anInt")).collect()
  [Row(anInt=1), Row(anInt=2), Row(anInt=3)]
  
  >>> eDF.select(explode(eDF.mapfield).alias("key", "value")).show()
  +---+-----+
  |key|value|
  +---+-----+
  |  a|    b|
  +---+-----+
  ```

- `explode_outer`:

  ```python
  # Returns hardware new row for each element in the given array or map. Unlike explode, if the array/map is null or empty then null is produced.
  # 对数组中每个元素返回一个新航. 不过和explode不同的是, 如果数据中的数组或映射字段为None或空, 则会填充None到数据的这个字段中.
  
  >>> df = spark.createDataFrame(
  ...     [(1, ["foo", "bar"], {"x": 1.0}), (2, [], {}), (3, None, None)],
  ...     ("id", "an_array", "a_map")
  ... )
  >>> df.select("id", "an_array", explode_outer("a_map")).show()
  +---+----------+----+-----+
  | id|  an_array| key|value|
  +---+----------+----+-----+
  |  1|[foo, bar]|   x|  1.0|
  |  2|        []|null| null|
  |  3|      null|null| null|
  +---+----------+----+-----+
  
  >>> df.select("id", "a_map", explode_outer("an_array")).show()
  +---+----------+----+
  | id|     a_map| col|
  +---+----------+----+
  |  1|[x -> 1.0]| foo|
  |  1|[x -> 1.0]| bar|
  |  2|        []|null|
  |  3|      null|null|
  +---+----------+----+
  ```

根据官方文档的释义, 数据量与实际不匹配(减少)的原因便显而易见了. 是因为生产数据中有很多数据中并没有需要打平的数组字段. 导致使用`explode`打平时, 无数组字段的数据都不见了.

#### 总结

所以, 后续如果需要对数据中的数组字段进行打平, 首先应该了解操作数据的实际情况. 数组字段是否必存在, 还是说并非必须存在. 根据数据的实际情况, 选择对应的函数进行处理, 避免像我一样, 出现数据量不匹配的问题而着急忙慌的.



