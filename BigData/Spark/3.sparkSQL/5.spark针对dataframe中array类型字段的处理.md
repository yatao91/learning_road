## spark针对dataframe中array类型字段的处理

#### 业务背景:

业务上需要对生产数据使用spark进行预计算. 而生产数据中存在数组类型字段, 之前的MySQL预计算方案是仅获取数组字段的首个元素作为目标值. 

但在使用spark进行处理时, 遇到了一些问题, 走了一些弯路, 特此记录一下.

#### 弯路概述

1. 对存在数组字段的数据使用`explode`打平

   当时, 考虑到数据为嵌套结构(JSON), 且存在数组字段, 首先考虑到的是进行打平(flatten)操作. 当时查阅了spark的相关文档, 然后采用了函数`explode`进行打平操作. 而且打平后的数据结构也是符合预期的. 但是出现了数据量不匹配的问题. 

   具体原因在于使用的函数`explode`, 在对数组进行打平操作时, 对无数组字段的数据, 将会进行抛弃. 对有多个元素的数组, 会拆分成多行. 既会造成数据量的减少, 又会造成数据量的增多. 

2. 对存在数组字段的数据使用`explode_outer`打平

   考虑到`explode`在打平时, 丢弃了无数组字段的数据, 导致数据量不匹配. 然后我又选用了这个函数, 因为这个函数在对数组字段进行打平操作时, 针对无数组字段的数据会进行None补全. 即将打平后的该数组字段置为None. 在我的业务上是满足要求的. 但同样存在数据量不匹配的问题. 

   `explode_outer`同`explode`一样, 都会对数组中各个元素生成多行数据, 导致数据量增大. 在我们的业务上, 这是不被允许的, 会造成统计结果的失真. 所以此种方案也否掉了.

#### 思路回顾

前期考虑对数据打平, 其实并不是首先考虑的数组字段, 而是考虑到字段嵌套问题. 当时考虑需要在`dataframe`上进行临时表的注册. 但嵌套结构的`SQL`语句写起来比较麻烦(本人很懒), 所以考虑直接使用`dataframe`提供的一些api来先打平, 然后在打平后的`dataframe`跑SQL语句来生成新的`dataframe`来注册临时表.

所以, 在进行打平过程中, 就遇到了数组打平的一系列问题, 然后思路也一直停留在既然有数组, 那就先干掉(打平)数组的路上了. 导致越走越远, 而忽略的最简单的解决方式

#### 方案调整

考虑到`dataframe`中的数组类型字段, 是否也可以向python中的`list`一样可以进行分片/根据索引获取元素操作呢? 

答案是肯定的. 

所以结合业务原则---采用数组的第一个元素作为有效值. 因此, 我们可以使用获取数组字段的第一个元素, 来解决数组字段的问题. 

查阅spark文档, 发现有这么个方法:

```python
getItem(key)[source]

# An expression that gets an item at position ordinal out of hardware list, or gets an item by key out of hardware dict.

>>> df = spark.createDataFrame([([1, 2], {"key": "value"})], ["l", "d"])
>>> df.select(df.l.getItem(0), df.d.getItem("key")).show()
+----+------+
|l[0]|d[key]|
+----+------+
|   1| value|
+----+------+

>>> df.select(df.l[0], df.d["key"]).show()
+----+------+
|l[0]|d[key]|
+----+------+
|   1| value|
+----+------+
```

从上面的示例代码可以看出: `getItem(key)`方法, 不仅可以获取数组的第一个元素, 也可以用来获取字典的某个key. 

> **注意**: 可能有人会问, 如果某条数据没有此数组字段怎么办? 按照python的列表的操作, 岂不会会报错? 
>
> 其实并不会, spark这里的getItem方法会去尝试获取某索引元素, 但当字段不存在, 则会自动补充None. 即并不会有什么异常情况出现. 具体实现的话就不细究了, 用得好就行.

#### 总结

解决问题时, 思路还是最重要的, 如果之前的思路被误导, 后面会走很多弯路.

