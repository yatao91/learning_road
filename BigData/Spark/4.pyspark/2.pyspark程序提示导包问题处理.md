## pyspark程序提示导包问题处理

#### 问题背景

近期在调研使用spark重构MySQL预计算部分. 

因公司技术栈以python为主, 调研的主要方向是pyspark的使用.

但在使用pyspark编写程序后, 并通过spark-submit提交spark节点运行时, 提示如下报错:

```python
$ spark-submit --master "local[*]" test_python_version.py

20/01/17 10:09:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Traceback (most recent call last):
  File "/home/spark/biplatform/app/tasks/test_python_version.py", line 11, in <module>
    from elasticsearch import Elasticsearch
  File "/home/spark/miniconda3/lib/python3.7/site-packages/elasticsearch/__init__.py", line 18, in <module>
    from .client import Elasticsearch
  File "/home/spark/miniconda3/lib/python3.7/site-packages/elasticsearch/client/__init__.py", line 4, in <module>
    from ..transport import Transport
  File "/home/spark/miniconda3/lib/python3.7/site-packages/elasticsearch/transport.py", line 4, in <module>
    from .connection import Urllib3HttpConnection
  File "/home/spark/miniconda3/lib/python3.7/site-packages/elasticsearch/connection/__init__.py", line 3, in <module>
    from .http_urllib3 import Urllib3HttpConnection, create_ssl_context
  File "/home/spark/miniconda3/lib/python3.7/site-packages/elasticsearch/connection/http_urllib3.py", line 3, in <module>
    import urllib3
  File "/home/spark/miniconda3/lib/python3.7/site-packages/urllib3/__init__.py", line 7, in <module>
    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url
  File "/home/spark/miniconda3/lib/python3.7/site-packages/urllib3/connectionpool.py", line 11, in <module>
    from .exceptions import (
  File "/home/spark/miniconda3/lib/python3.7/site-packages/urllib3/exceptions.py", line 2, in <module>
    from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead
  File "/home/spark/miniconda3/lib/python3.7/site-packages/urllib3/packages/six.py", line 199, in load_module
    mod = mod._resolve()
  File "/home/spark/miniconda3/lib/python3.7/site-packages/urllib3/packages/six.py", line 113, in _resolve
    return _import_module(self.mod)
  File "/home/spark/miniconda3/lib/python3.7/site-packages/urllib3/packages/six.py", line 82, in _import_module
    __import__(name)
  File "/home/spark/miniconda3/lib/python3.7/http/client.py", line 71, in <module>
    import email.parser
  File "/home/spark/biplatform/app/tasks/email.py", line 5, in <module>
    from app.services.email_service import email_server
ModuleNotFoundError: No module named 'app'
```

提示`ModuleNotFoundError: No module named 'app'`错误.

此spark计算程序是放在BI项目的某个子目录中, 按理说不会出现此问题, 虽然计算程序中引入了第三方库, 但Python环境中(不管是driver还是executor节点)均安装了第三方库. 网上搜索并没有什么与此异常有关的信息. 大多是关于提交计算程序到集群模式下时的环境统一问题.

#### 问题解决思路

经过对异常堆栈的分析:

```python
  File "/home/spark/miniconda3/lib/python3.7/http/client.py", line 71, in <module>
    import email.parser
  File "/home/spark/biplatform/app/tasks/email.py", line 5, in <module>
    from app.services.email_service import email_server
ModuleNotFoundError: No module named 'app'
```

当进行`import email.parser` 时, 因项目路径下存在`email.py` , 导致与第三方包中依赖的python模块名重复. 

当将项目路径下的`email.py` 重命名后, 此问题得到解决.

#### 教训

追踪问题时, 仔细思考异常堆栈信息, 发现蛛丝马迹, 进行解决.