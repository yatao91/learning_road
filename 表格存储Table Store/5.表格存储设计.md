+ 注意点: 

  + 主键中key和列中key名字不能重复
+ 插入时候,  设置成同主键数据不能多次插入成功, 非首次插入数据有报错异常.
  + 属性行数据最多是1m, 如果rawhtml大于1m, 有两种方法可以处理:
  + 将rawhtml存入oss,  需要自己实现事务,
    + 用多行写入的方式,  不需要事务, 需要手动分割rawhtml(有可能是多个rawhtml). 读取的时候需要拼接rawhtml

+ 主键设计:

  "{prefix:int}:{原来数据库表名:str}:{发布时间:str}:{url-hash:str}", 

  其中: 

  prefix前缀是用于数据分区的标识,  ggzy 2018年的数据有465g, 阿里云建议每个分区数据大小是10g.

  所以我的思路是: 将url的md5值(16进制的32位字符串)转换为10进制数字, 跟奇数99求余数, 可以得到0-99共100个分区数, 每个分区容量是10g, 总共1000g也就是1t数据.

  新方法: 用murmur3  hash生成一个数字

  样例:

  ```
  99:ccgp-province-shandong-citye-jinan-ccgp-shandong-com:2019-01-01:3820cf7cf5ac49fcc22137adba723da9
  ```

  

+ 表设计

  + 表需要手动在阿里云网页上创建, 有一分钟的生效时间, 不能用爬虫程序自动生成

  + 思路:  "{原来数据库表名:str}"

  + 样例:

    ```
    ccgp-city
    ccgp-province
    ggzy-country
    ```

  + 建一个专门存储超过2m单列数据html的表`more-rawhtml`  起主键设计思路是:

    + "{prefix:int}:{url-hash:str}:{发布时间-str}:{分割的顺序数:int}"
    + 写入多行html, 需要用到事务, 参考 https://help.aliyun.com/document_detail/93819.html

+ 属性列设计

  + publishTime: 发布时间, 

  + crawlerTime: 爬取时间, 

  + url: 具体url

  + title: 标题

  + tr: 行

  + category: 具体分类, 比如 中标, 招标

  + fileUrlList:  附件url列表的json

  + rawHtml: 原始html

  + rawHtmlEnd:  是否有分割的html

  + rawHtmlKeyList:  分割后每一个html存储的key的list的json, 下面每一个rawHtml都是同样设计

  + rawHtml01: 原网站中需要点击再次打开的html

  + rawHtml02: 原网站中需要点击再次打开的html

  + rawHtml03: 原网站中需要点击再次打开的html

  + rawHtml04: 原网站中需要点击再次打开的html

  + rawHtml05: 原网站中需要点击再次打开的html

  + rawHtml06: 原网站中需要点击再次打开的html

  + rawHtml07: 原网站中需要点击再次打开的html

  + rawHtml08: 原网站中需要点击再次打开的html

  + rawHtml09: 原网站中需要点击再次打开的html

  + rawHtml10: 原网站中需要点击再次打开的html

    

    

     

+ 防止表格存储列数据的key名不一致, 需要事先定义好数据模型, mysql数据表如果 key名不一致会报错, 表格存储可以存任意格式的数据

  ```
  class SpiderModel:
      """
      domain数据类型,
      每个数据列的具体类型,
      普通爬虫人员不能修改,
      尽可能
      """
      publishTime = None
      crawlerTime = None
      url = ""
      rawHtml = ""
  
  
  def make() -> SpiderModel:
      """
      写爬虫人员先可以修改和赋值的部分
      :return:
      """
      spider_model = SpiderModel()
      spider_model.publishTime = "2019-01-01"
      spider_model.crawlerTime = "2019-03-01"
      return spider_model
  
  
  def main():
      """
      base里面确定, 普通爬虫人员不能修改
      :return:
      """
      model = make()
      # table_data是写入列数据的具体数据
      # 加判断, 如果model 数据的值为空, 不用写入表格存储
      table_data = [
          ("publishTime", model.publishTime),
          ("crawlerTime", model.crawlerTime)
      ]
  
      print(table_data)
  
  
  if __name__ == '__main__':
      main()
  ```

+ 范围读取

  + 参考 https://help.aliyun.com/document_detail/27298.html
  + 表格存储的范围读取是按顺序从上到下扫描数据.
  + 表格存储表中的行按主键进行从小到大排序. 我们设计的第一个主键是url的murmur3 hash后的数值, 是未知的. 所以需要扫描全表.  发布时间最小的未必在最上面一行.
  + 需要获取发布时间某一天的数据, 所有pk都设置成最大和最小. 用属性列的条件过滤.

  ```
      def xget_range(self):
      
      	"""
      	python迭代获取符合条件的所有数据
      	"""
          consumed_counter = CapacityUnit(0, 0)
          inclusive_start_primary_key = [('prefix', INF_MIN), ('table_name', INF_MIN), ('publish_time', INF_MIN),
                                         ('url', INF_MIN)]
          exclusive_end_primary_key = [('prefix', INF_MAX), ('table_name', INF_MAX), ('publish_time', INF_MAX),
                                       ('url', INF_MAX)]
  
          cond = CompositeColumnCondition(LogicalOperator.AND)
          cond.add_sub_condition(SingleColumnCondition("publishTime", '12/4/2019 00:00:00', ComparatorType.EQUAL))
          cond.add_sub_condition(SingleColumnCondition("publishTime", '12/4/2019 00:00:00', ComparatorType.EQUAL))
  		# 可以设置获取属性列的某几个key
          columns_to_get = []
          range_iter = self.client.xget_range(
              table_name, Direction.FORWARD,
              inclusive_start_primary_key, exclusive_end_primary_key,
              consumed_counter, columns_to_get,
              column_filter=cond, max_version=1
          )
          total_rows = 0
          for row in range_iter:
              print(row.primary_key, row.attribute_columns)
              total_rows += 1
  
          print('Total rows:', total_rows)
  ```

  